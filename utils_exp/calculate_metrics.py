"""
Metrics calculation utilities for time series predictions.

This module provides functions to calculate various forecasting metrics (MSE, RMSE,
MAE, MASE, R2, MAPE, SMAPE) from the predictions DataFrame generated by 
_save_predictions in MLExperimentFacade.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional, Dict, List
import logging

logger = logging.getLogger(__name__)


def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Calculate Mean Squared Error."""
    return np.mean((y_true - y_pred) ** 2)


def root_mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Calculate Root Mean Squared Error."""
    return np.sqrt(mean_squared_error(y_true, y_pred))


def mean_absolute_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Calculate Mean Absolute Error."""
    return np.mean(np.abs(y_true - y_pred))


def mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Calculate Mean Absolute Percentage Error.
    Handles zero values by adding a small epsilon.
    """
    epsilon = 1e-10
    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100


def symmetric_mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Calculate Symmetric Mean Absolute Percentage Error.
    More robust to zero values than MAPE.
    """
    numerator = np.abs(y_true - y_pred)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
    # Avoid division by zero
    mask = denominator > 0
    if not mask.any():
        return np.nan
    return np.mean(numerator[mask] / denominator[mask]) * 100


def mean_absolute_scaled_error(
    y_true: np.ndarray, 
    y_pred: np.ndarray, 
    y_train: np.ndarray,
    seasonality: int = 1
) -> float:
    """
    Calculate Mean Absolute Scaled Error.
    
    Parameters
    ----------
    y_true
        Actual values
    y_pred
        Predicted values
    y_train
        Training data for calculating naive forecast error
    seasonality
        Seasonal period (1 for non-seasonal data)
    """
    mae = mean_absolute_error(y_true, y_pred)
    
    # Calculate MAE of naive seasonal forecast on training data
    if len(y_train) <= seasonality:
        logger.warning("Training data too short for MASE calculation")
        return np.nan
    
    naive_error = np.mean(np.abs(np.diff(y_train, n=seasonality)))
    
    if naive_error == 0:
        return np.nan
    
    return mae / naive_error


def r2_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Calculate R² (coefficient of determination).
    
    R² = 1 - (SS_res / SS_tot)
    where SS_res = sum of squared residuals
          SS_tot = total sum of squares
    """
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    
    if ss_tot == 0:
        return np.nan
    
    return 1 - (ss_res / ss_tot)


def calculate_all_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_train: Optional[np.ndarray] = None,
    seasonality: int = 1,
) -> Dict[str, float]:
    """
    Calculate all available metrics.
    
    Parameters
    ----------
    y_true
        Actual values
    y_pred
        Predicted values
    y_train
        Training data (optional, needed for MASE)
    seasonality
        Seasonal period for MASE calculation
        
    Returns
    -------
    Dictionary with all calculated metrics
    """
    # Filter out any NaN values
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true_clean = y_true[mask]
    y_pred_clean = y_pred[mask]
    
    if len(y_true_clean) == 0:
        logger.warning("No valid data points for metric calculation")
        return {
            "MSE": np.nan,
            "RMSE": np.nan,
            "MAE": np.nan,
            "MAPE": np.nan,
            "SMAPE": np.nan,
            "R2": np.nan,
            "MASE": np.nan,
            "n_samples": 0,
        }
    
    metrics = {
        "MSE": mean_squared_error(y_true_clean, y_pred_clean),
        "RMSE": root_mean_squared_error(y_true_clean, y_pred_clean),
        "MAE": mean_absolute_error(y_true_clean, y_pred_clean),
        "MAPE": mean_absolute_percentage_error(y_true_clean, y_pred_clean),
        "SMAPE": symmetric_mean_absolute_percentage_error(y_true_clean, y_pred_clean),
        "R2": r2_score(y_true_clean, y_pred_clean),
        "n_samples": len(y_true_clean),
    }
    
    # Calculate MASE if training data is available
    if y_train is not None and len(y_train) > seasonality:
        y_train_clean = y_train[~np.isnan(y_train)]
        metrics["MASE"] = mean_absolute_scaled_error(
            y_true_clean, y_pred_clean, y_train_clean, seasonality
        )
    else:
        metrics["MASE"] = np.nan
    
    return metrics


def calculate_metrics_from_predictions(
    predictions_df: pd.DataFrame,
    group_by: Optional[List[str]] = None,
    seasonality: int = 1,
) -> pd.DataFrame:
    """
    Calculate metrics from predictions DataFrame.
    
    Parameters
    ----------
    predictions_df
        DataFrame from _save_predictions with columns:
        - actual_value: Ground truth
        - predicted_mean: Model predictions
        - Additional grouping columns (model_name, experiment_name, item_id, etc.)
    group_by
        List of columns to group by before calculating metrics.
        If None, calculates overall metrics.
        Common options: ["model_name"], ["model_name", "experiment_name"],
                       ["model_name", "item_id"], etc.
    seasonality
        Seasonal period for MASE calculation
        
    Returns
    -------
    DataFrame with metrics for each group
    """
    logger.info("Calculating metrics from predictions")
    
    if group_by is None:
        # Calculate overall metrics
        y_true = predictions_df["actual_value"].values
        y_pred = predictions_df["predicted_mean"].values
        
        metrics = calculate_all_metrics(y_true, y_pred, seasonality=seasonality)
        return pd.DataFrame([metrics])
    
    # Calculate metrics per group
    results = []
    
    grouped = predictions_df.groupby(group_by)
    total_groups = len(grouped)
    
    logger.info(f"Calculating metrics for {total_groups} groups")
    
    for idx, (group_keys, group_data) in enumerate(grouped, 1):
        if idx % 10 == 0:
            logger.info(f"Processing group {idx}/{total_groups}")
        
        y_true = group_data["actual_value"].values
        y_pred = group_data["predicted_mean"].values
        
        metrics = calculate_all_metrics(y_true, y_pred, seasonality=seasonality)
        
        # Add group identifiers
        if isinstance(group_keys, tuple):
            for col, val in zip(group_by, group_keys):
                metrics[col] = val
        else:
            metrics[group_by[0]] = group_keys
        
        results.append(metrics)
    
    return pd.DataFrame(results)


def generate_metrics_report(
    predictions_path: Path,
    output_path: Optional[Path] = None,
    seasonality: int = 1,
) -> pd.DataFrame:
    """
    Generate comprehensive metrics report from predictions file.
    
    Calculates metrics at multiple aggregation levels:
    1. Overall (all data)
    2. Per model
    3. Per model + experiment
    4. Per model + item_id
    5. Per model + experiment + item_id
    
    Parameters
    ----------
    predictions_path
        Path to predictions.parquet file
    output_path
        Path to save metrics CSV. If None, doesn't save.
    seasonality
        Seasonal period for MASE calculation
        
    Returns
    -------
    DataFrame with all metrics at different aggregation levels
    """
    logger.info(f"Loading predictions from {predictions_path}")
    predictions_df = pd.read_parquet(predictions_path)
    
    logger.info(f"Loaded {len(predictions_df)} prediction records")
    
    all_metrics = []
    
    # 1. Overall metrics
    logger.info("Calculating overall metrics")
    overall = calculate_metrics_from_predictions(predictions_df, group_by=None, seasonality=seasonality)
    overall["aggregation_level"] = "overall"
    all_metrics.append(overall)
    
    # 2. Per model
    logger.info("Calculating metrics per model")
    per_model = calculate_metrics_from_predictions(
        predictions_df, group_by=["model_name"], seasonality=seasonality
    )
    per_model["aggregation_level"] = "per_model"
    all_metrics.append(per_model)
    
    # 3. Per model + experiment
    if "experiment_name" in predictions_df.columns:
        logger.info("Calculating metrics per model + experiment")
        per_model_exp = calculate_metrics_from_predictions(
            predictions_df, group_by=["model_name", "experiment_name"], seasonality=seasonality
        )
        per_model_exp["aggregation_level"] = "per_model_experiment"
        all_metrics.append(per_model_exp)
    
    # 4. Per model + item_id
    if "item_id" in predictions_df.columns:
        logger.info("Calculating metrics per model + item_id")
        per_model_item = calculate_metrics_from_predictions(
            predictions_df, group_by=["model_name", "item_id"], seasonality=seasonality
        )
        per_model_item["aggregation_level"] = "per_model_item"
        all_metrics.append(per_model_item)
    
    # 5. Per model + experiment + item_id
    if "experiment_name" in predictions_df.columns and "item_id" in predictions_df.columns:
        logger.info("Calculating metrics per model + experiment + item_id")
        per_all = calculate_metrics_from_predictions(
            predictions_df,
            group_by=["model_name", "experiment_name", "item_id"],
            seasonality=seasonality,
        )
        per_all["aggregation_level"] = "per_model_experiment_item"
        all_metrics.append(per_all)
    
    # 6. Per model + context_length + prediction_length
    if "context_length" in predictions_df.columns and "prediction_length" in predictions_df.columns:
        logger.info("Calculating metrics per model + context + prediction length")
        per_config = calculate_metrics_from_predictions(
            predictions_df,
            group_by=["model_name", "context_length", "prediction_length"],
            seasonality=seasonality,
        )
        per_config["aggregation_level"] = "per_model_config"
        all_metrics.append(per_config)
    
    # Combine all metrics
    metrics_df = pd.concat(all_metrics, ignore_index=True)
    
    # Reorder columns for better readability
    metric_cols = ["MSE", "RMSE", "MAE", "MAPE", "SMAPE", "R2", "MASE", "n_samples"]
    other_cols = [col for col in metrics_df.columns if col not in metric_cols]
    metrics_df = metrics_df[other_cols + metric_cols]
    
    logger.info(f"Generated {len(metrics_df)} metric rows")
    
    # Save to file if requested
    if output_path:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        metrics_df.to_csv(output_path, index=False)
        logger.info(f"Saved metrics to {output_path}")
    
    return metrics_df


def print_metrics_summary(metrics_df: pd.DataFrame) -> None:
    """
    Print a formatted summary of metrics.
    
    Parameters
    ----------
    metrics_df
        DataFrame with calculated metrics
    """
    print("\n" + "=" * 80)
    print("METRICS SUMMARY")
    print("=" * 80)
    
    # Group by aggregation level
    for level in metrics_df["aggregation_level"].unique():
        level_data = metrics_df[metrics_df["aggregation_level"] == level]
        
        print(f"\n{level.upper().replace('_', ' ')}")
        print("-" * 80)
        
        # Select relevant columns for display
        display_cols = [col for col in level_data.columns if col not in ["aggregation_level"]]
        
        # Format numbers for better readability
        pd.options.display.float_format = "{:.4f}".format
        print(level_data[display_cols].to_string(index=False))
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    import sys
    import argparse
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )
    
    parser = argparse.ArgumentParser(
        description="Calculate forecasting metrics from predictions parquet file"
    )
    parser.add_argument(
        "predictions_path",
        type=str,
        help="Path to predictions.parquet file",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=str,
        default=None,
        help="Path to save metrics CSV file (default: same directory as predictions)",
    )
    parser.add_argument(
        "--seasonality",
        "-s",
        type=int,
        default=1,
        help="Seasonal period for MASE calculation (default: 1)",
    )
    parser.add_argument(
        "--no-summary",
        action="store_true",
        help="Don't print summary to console",
    )
    
    args = parser.parse_args()
    
    predictions_path = Path(args.predictions_path)
    
    # Default output path
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = predictions_path.parent / "metrics_report.csv"
    
    # Generate metrics
    metrics_df = generate_metrics_report(
        predictions_path,
        output_path=output_path,
        seasonality=args.seasonality,
    )
    
    # Print summary
    if not args.no_summary:
        print_metrics_summary(metrics_df)
    
    print(f"\nMetrics saved to: {output_path}")
